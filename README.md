# **Multilayer Perceptron Project**

:warning: **Warning:**
This README was **partially generated by ChatGPT-4o**. While effort has been made to ensure accuracy, some details may be incorrect or require further verification.
Please **double-check the information**.

---

## **Project Overview**
This project focuses on the implementation of a **Multilayer Perceptron (MLP)** from scratch, a fundamental type of artificial neural network. The goal is to design and train a neural network capable of predicting whether a tumor is **malignant or benign** based on the **Wisconsin Breast Cancer dataset**.

Unlike using pre-built machine learning frameworks, this project requires implementing core neural network algorithms, including **forward propagation, backpropagation, and gradient descent**, without leveraging deep learning libraries. This hands-on approach helps deepen the understanding of the mathematical foundations behind artificial neural networks.

## **Objectives**
The primary objectives of this project are:

✅ **Understand and implement artificial neural networks (ANNs):** Gain a strong foundation in neural networks by coding an MLP from scratch.  
✅ **Train a model for binary classification:** Predict tumor malignancy based on medical features extracted from cell nuclei.  
✅ **Implement core learning algorithms:** Develop the essential components of a neural network, including:
- **Feedforward propagation** to compute predictions.
- **Backpropagation** to update network weights.
- **Gradient descent optimization** for model training.  
  ✅ **Work with real-world medical data:** Process and analyze the **Wisconsin Breast Cancer dataset** to extract meaningful insights.  
  ✅ **Evaluate model performance:** Use **loss functions** (such as cross-entropy loss) and performance metrics to assess accuracy.  
  ✅ **Visualize learning progress:** Implement **learning curve graphs** to track loss and accuracy trends over time.

This project serves as an introduction to **deep learning** concepts and paves the way for more advanced neural network architectures in future studies.

---

## **Project Documentation**

[Auto-generated documentation from Docstrings](https://brenaudon.github.io/Multilayer_Perceptron/)

---

## **Usage**

This project consists of multiple scripts designed to preprocess data, train a neural network, visualize results, and make predictions. Below are the usage instructions for each script.

### 🔹 **1. Visualizing Data**
To explore the dataset visually, use the `visualize.py` script.

```bash
python visualize.py -d <csv_file_path>
```
- `-d, --dataset` : Path to the dataset CSV file.

This script generates:
- **Histograms** for each feature.
- **Pair plots** to analyze feature relationships.

### 🔹 **2. Splitting the Dataset**
Before training, the dataset should be split into **training** and **validation** sets.

```bash
python cut_dataset.py <csv_file_path> <percentage>
```
- `<csv_file_path>`: Path to the dataset CSV file.
- `<percentage>`: Percentage of data to use for training (0-100).

**Example:**
```bash
python cut_dataset.py data.csv 80
```
This will split `data.csv` into 80% training and 20% validation.

### 🔹 **3. Training the Model**
To train the **multilayer perceptron**, use the `training.py` script.

```bash
python training.py -d <dataset_csv> -c <config_json> [-s <save>]
```
- `-d, --dataset` : Path to the dataset CSV file.
- `-c, --config`  : Path to the configuration JSON file.
- `-s, --save` *(optional)* : Whether to save the training and validation sets after splitting.

**Example:**
```bash
python training.py -d data.csv -c config.json -s True
```
This will train the model using `data.csv` and the settings from `config.json`.

### 🔹 **4. Making Predictions**
Once trained, the model can make predictions on new data.

```bash
python predict.py -d <data_csv> -m <model_name>
```
- `-d, --data` : Path to the CSV data file.
- `-m, --model` : Name of the trained model.

**Example:**
```bash
python predict.py -d new_data.csv -m my_model
```
This will use the trained model `my_model` to make predictions on `new_data.csv`.

### 🔹 **5. Comparing Model Performance**
To compare multiple trained models:

```bash
python models_comparison.py [model1 model2 ...]
```
- If no models are specified, all available models are plotted.

**Example:**
```bash
python models_comparison.py model_A model_B
```
This will generate comparison plots for `model_A` and `model_B`.

---

### **Configuration Files: Usage and Fields**

To customize the neural network's structure, training parameters, and optimization settings, this project uses **JSON configuration files**. These files define the number of layers, activation functions, training hyperparameters, and performance metrics.

#### **Usage of Configuration Files**
When running the **training script**, the configuration file must be provided as an argument:

```bash
python training.py -d data.csv -c config.json
```
- `-d <dataset_csv>`: Path to the dataset CSV file.
- `-c <config_json>`: Path to the JSON configuration file containing model settings.

## **Configuration File**

The **JSON configuration file** is used to define the neural network architecture, learning rate scheduling, optimizer parameters, and general training settings. This structured approach allows easy customization of the model architecture and hyperparameters.

Except for **nb_neurons** in each layer, all other fields are **optional** and have default values.

### **Fields in the Configuration File**

#### **1. Defining Network Layers (`layerX`)**
Each layer in the neural network is defined using `layerX` keys, where `X` is the layer number. `X` must start from `1` and increment sequentially.
A minimum of **two hidden layers** is required.
Output layer is fixed with a softmax activation function as required in the subject.

| **Field**          | **Type** | **Description**                                                                                                                                                                                                      |
|--------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **layerX**         | `dict`   | Defines a layer in the neural network (`X` is the layer number).                                                                                                                                                     |
| ├ `nb_neurons`     | `int`    | Number of neurons in the layer.                                                                                                                                                                                      |
| ├ `activation`     | `string` | Activation function *(possible: `"sigmoid"`, `"tanh"`, `"relu"`, `"leaky_relu"`, `"elu"`, `"selu"`, `"swish"`, `"gelu"`)*.                                                                                           |
| ├ `initialization` | `string` | Weight initialization method *(possible: `"random_normal"`, `"random_uniform"`, `"he_normal"`, `"he_uniform"`, `"xavier_glorot_normal"`, `"xavier_glorot_uniform"`, `"lecun_normal"`, `"lecun_uniform"`, `"selu"`)*. |

Please see the **Activation Functions** and **Initialization Functions** sections for more details on activation functions and weight initialization methods.

📌 **Example Layer Definition**:
```json
"layer1": {
    "nb_neurons": 36,
    "activation": "tanh",
    "initialization": "random_normal"
}
```

### **2. Learning Rate Scheduling (`schedule_params`)**
The learning rate can be adjusted dynamically using scheduling methods.

| **Field**                  | **Type**   | **Description**                                                                                     |
|----------------------------|------------|-----------------------------------------------------------------------------------------------------|
| **schedule**               | `string`   | Type of learning rate schedule *(possible: `"step"`, `"exponential"`, `"cosine"`, `"time_based"`)*. |
| **schedule_params**        | `dict`     | Contains parameters for the selected learning rate schedule.                                        |
| ├ `initial_learning_rate`  | `float`    | Initial learning rate value.                                                                        |
| ├ `decay_rate`             | `float`    | *(Only for `"exponential"` and `"time_based"` schedules)* Decay rate applied to the learning rate.  |
| ├ `drop_factor`            | `float`    | *(Only for `"step"` schedule)* Factor by which the learning rate is reduced.                        |
| ├ `epochs_drop`            | `int`      | *(Only for `"step"` schedule)* Number of epochs before reducing learning rate.                      |

Please see the **Learning Rate Schedules** section for more details on different scheduling methods.

📌 **Example Learning Rate Schedule**:
```json
"schedule": "exponential",
"schedule_params": {
    "initial_learning_rate": 0.001,
    "decay_rate": 0.05
}
```

### **3. Optimizer Parameters (`optimizer_params`)**
The optimization algorithm used during training can be configured.

| **Field**            | **Type**   | **Description**                                                                                             |
|----------------------|------------|-------------------------------------------------------------------------------------------------------------|
| **optimization**     | `string`   | Optimizer type *(e.g., `"gradient_descent"`, `"adagrad"`, `"momentum"`, `"rmsprop"`, `"adam"`, `"adamw"`)*. |
| **optimizer_params** | `dict`     | Additional optimizer-specific parameters.                                                                   |
| ├ `beta`             | `float`    | *(For `"momentum"` and `"rmsprop"` optimizers)* Momentum factor.                                            |
| ├ `beta1`            | `float`    | *(For `"adam"` and `"adamw"` optimizers)* First moment decay rate.                                          |
| ├ `beta2`            | `float`    | *(For `"adam"` and `"adamw"` optimizers)* Second moment decay rate.                                         |
| ├ `weight_decay`     | `float`    | *(For `"adamw"` optimizer)* Weight decay for regularization.                                                |

Please see the **Optimization Algorithms** section for more details on different optimizer types.

📌 **Example Optimizer Configuration**:
```json
"optimization": "adamw",
"optimizer_params": {
    "beta1": 0.9,
    "beta2": 0.999,
    "weight_decay": 0.01
}
```

### **4. General Training Parameters**
These settings define batch size, epochs, and evaluation metrics.

| **Field**                   | **Type** | **Description**                                                                                             |
|-----------------------------|----------|-------------------------------------------------------------------------------------------------------------|
| **batch_size**              | `int`    | Number of samples per training batch.                                                                       |
| **epochs**                  | `int`    | Number of training iterations over the entire dataset.                                                      |
| **learning_rate**           | `float`  | *(Only used if no `schedule` is defined)* Fixed learning rate.                                              |
| **l1_lambda**               | `float`  | L1 regularization parameter to prevent overfitting.                                                         |
| **l2_lambda**               | `float`  | L2 regularization parameter to prevent overfitting.                                                         |
| **dropout_rate**            | `float`  | Dropout rate to turn off a part of each layer neurons during training to prevent overfitting                |
| **early_stopping_patience** | `int`    | Early stopping patience (in epochs) before stopping training. No early stopping if none                     |
| **metrics**                 | `list`   | Evaluation metrics *(possible: `"accuracy"`, `"precision"`, `"recall"`, `"f1"`, `"roc_auc"`, `"pr_auc"`)*.  |
| **model_name**              | `string` | Name of the model (used for saving/loading).                                                                |
| **display**                 | `string` | Display mode *(`"tqdm"` for progress bar visualization during training. Subject required display if none).* |

Please see the **Evaluation Metrics** section for more details on different performance metrics. See **L1/L2 Regularization**, **Dropout** and **Early Stopping** sections for more details on these techniques.

📌 **Example General Training Parameters**:
```json
"batch_size": 8,
"epochs": 1500,
"metrics": ["accuracy", "precision", "recall", "f1", "roc_auc", "pr_auc"],
"model_name": "model",
"display": "tqdm"
```

### **Example Configuration Files**

```json
{
    "layer1": {
        "nb_neurons": 36,
        "activation": "tanh",
        "initialization": "random_normal"
    },
    "layer2": {
        "nb_neurons": 36,
        "activation": "tanh",
        "initialization": "random_normal"
    },
    "schedule": "exponential",
    "schedule_params": {
        "initial_learning_rate": 0.001,
        "decay_rate": 0.05
    },
    "optimization": "adamw",
    "optimizer_params": {
        "beta1": 0.9,
        "beta2": 0.999,
        "weight_decay": 0.01
    },
    "batch_size": 8,
    "epochs": 1500,
    "metrics": ["accuracy", "precision", "recall", "f1", "roc_auc", "pr_auc"],
    "model_name": "model",
    "display": "tqdm"
}
```

### **Choosing the Right Configuration**

Please refer to the **Activation Functions**, **Initialization Functions**, **Learning Rate Schedules**, and **Optimization Algorithms** sections to understand the impact of different settings on neural network training.
Some may be more suitable for specific tasks or architectures, so it's essential to choose the right configuration for the project project.

---

## **Dataset Description**

The dataset used in this project is the **Wisconsin Breast Cancer Dataset (WBCD)**, a well-known dataset in the machine learning community for binary classification tasks. It consists of **features extracted from fine-needle aspiration (FNA) of breast masses**, aiming to classify tumors as **malignant (M)** or **benign (B)**.

### **Features**
The dataset contains **32 columns**, including:
- **ID**: A unique identifier for each sample.
- **Diagnosis**: The target variable, with values **M** (malignant) or **B** (benign).
- **30 numerical features** describing characteristics of the **cell nuclei**, such as:
  - **Radius**: Mean distance from the center to the perimeter.
  - **Texture**: Standard deviation of gray-scale values.
  - **Smoothness**: Variation in radius lengths.
  - **Compactness, Concavity, Symmetry, Fractal dimension**, etc.

### **Preprocessing Requirements**
Before using the dataset for training, the following preprocessing steps are necessary:  
✅ **Data Splitting**: The dataset is divided into **training** and **validation** sets (e.g., 80%-20%).  
✅ **Normalization**: Feature values are scaled to ensure consistent learning.  
✅ **Handling Missing Values**: The dataset should be checked for any missing or inconsistent values.  
✅ **Principal Component Analysis (PCA)** *(optional)*: Reduces feature dimensions while preserving key patterns.

By using this dataset, the neural network is trained to classify whether a tumor is **benign or malignant** based on the given attributes.

---

## **Activation Functions**

### 🔹 **1. ReLU (Rectified Linear Unit)**
#### **Definition**
ReLU is one of the most widely used activation functions in deep learning. It is defined as:

```math
f(Z) = \max(0, Z)
```

This means that for any negative input, the output is 0, and for any positive input, the output is the same as the input.

#### **Derivative**
The derivative of ReLU is simple:

```math
f'(Z) =
\begin{cases}
1, & Z > 0 \\
0, & Z \leq 0
\end{cases}
```

#### **Pros**
✅ Helps mitigate the **vanishing gradient problem**.  
✅ Computationally efficient (simple operations).  
✅ Works well in deep networks.

#### **Cons**
❌ **Dying ReLU problem**: If a neuron only gets negative inputs, it stops learning (gradient = 0).  
❌ Does not handle negative values well.

#### **Best Use Cases**
- Works well for deep networks.
- Recommended for **hidden layers** in most architectures.


### 🔹 **2. Leaky ReLU**
#### **Definition**
Leaky ReLU solves the **dying ReLU problem** by allowing a small slope for negative values:

```math
f(Z) = \max(0.01Z, Z)
```

This means that instead of setting negative values to **0**, we allow a small negative slope (usually **0.01**, but it can be tuned).

#### **Derivative**
```math
f'(Z) =
\begin{cases}
1, & Z > 0 \\
0.01, & Z \leq 0
\end{cases}
```

#### **Pros**
✅ Avoids **dying ReLU problem** (neurons always have some gradient).  
✅ Works better than ReLU when dealing with negative values.

#### **Cons**
❌ Slightly more computationally expensive than ReLU.

#### **Best Use Cases**
- Use **instead of ReLU** when you notice many neurons **dying (stuck at 0)**.


### 🔹 **3. ELU (Exponential Linear Unit)**
#### **Definition**
ELU is similar to Leaky ReLU but uses an **exponential curve** instead of a fixed slope for negative values:

```math
f(Z) =
\begin{cases}
Z, & Z > 0 \\
\alpha (e^Z - 1), & Z \leq 0
\end{cases}
```

where **α (alpha)** is usually set to **1**.

#### **Derivative**
```math
f'(Z) =
\begin{cases}
1, & Z > 0 \\
f(Z) + \alpha, & Z \leq 0
\end{cases}
```

#### **Pros**
✅ Avoids dying ReLU problem.  
✅ Allows small negative values, making it more robust than ReLU.  
✅ Can speed up learning.

#### **Cons**
❌ Slightly more computationally expensive.  
❌ Requires tuning the **α** parameter.

#### **Best Use Cases**
- Works well for deep networks where ReLU might struggle.


### 🔹 **4. SELU (Scaled Exponential Linear Unit)**
#### **Definition**
SELU (Scaled Exponential Linear Unit) is an activation function designed to **self-normalize** neural networks. It was introduced in the paper:  
👉 *Klambauer et al., 2017 - "Self-Normalizing Neural Networks"*

The SELU activation function is defined as:

```math
SELU(x) =
\begin{cases}
\lambda x & \text{if } x > 0 \\
\lambda \alpha (e^x - 1) & \text{if } x \leq 0
\end{cases}
```

Where:

- $\lambda \approx 1.0507$ (scaling factor)
- $\alpha \approx 1.67326$ (negative slope factor)


This scaling ensures that activations **automatically normalize** their mean and variance, helping deep networks stabilize during training.


#### **Derivative**
```math
SELU'(x) =
\begin{cases}
\lambda & \text{if } x > 0 \\
\lambda \alpha e^x & \text{if } x \leq 0
\end{cases}
```

#### **Pros**
✅ **Self-normalizing** — reduces the need for Batch Normalization.  
✅ **Helps with vanishing/exploding gradients** due to its smooth non-linearity.  
✅ **Better for deep networks** — prevents the network from suffering from vanishing gradients or saturation issues.  
✅ **Works well with LeCun Normal initialization and Alpha Dropout**.  
✅ **Can help improve convergence speed and stability in deep networks**.

#### **Cons**
❌ **Not effective in CNNs and RNNs** — works best for fully connected (dense) networks.  
❌ **Requires careful weight initialization (LeCun Normal)** and **Alpha Dropout**.  
❌ **Computationally more expensive** than simpler activation functions like ReLU, due to the exponential part for negative values.

#### **Best Use Cases**
- Deep **fully connected networks** (especially for classification and regression tasks).
- When you want **self-normalization** and stable training without extra normalization layers like BatchNorm.
- Networks where careful **weight initialization** (LeCun Normal) and **Alpha Dropout** are used to keep activations properly normalized.
- Tasks like **image classification**, **speech recognition**, or **NLP** (where deep, stable networks are crucial).


### 🔹 **5. Swish (Self-Gated)**
#### **Definition**
Swish is a **smooth, non-monotonic** function developed by Google that often outperforms ReLU:

```math
f(Z) = Z \cdot \sigma(Z) = \frac{Z}{1 + e^{-Z}}
```

where **σ(Z)** is the **sigmoid** function.

#### **Derivative**
```math
f'(Z) = \sigma(Z) + Z \cdot \sigma(Z) \cdot (1 - \sigma(Z))
```

#### **Pros**
✅ Helps prevent **dying neurons**.  
✅ Works well in **deep networks**.  
✅ Often outperforms ReLU.

#### **Cons**
❌ More computationally expensive than ReLU.

#### **Best Use Cases**
- Deep networks (especially for NLP and computer vision).
- If you want to **experiment** beyond ReLU.


### 🔹 **6. GELU (Gaussian Error Linear Unit)**
#### **Definition**
GELU is used in **BERT, GPT, and transformers**. Instead of using a simple threshold (like ReLU), it smoothly adjusts activations based on a Gaussian curve:

```math
f(Z) = 0.5 Z \left(1 + \tanh \left( \sqrt{\frac{2}{\pi}} \left( Z + 0.044715 Z^3 \right) \right) \right)
```

#### **Derivative**
The derivative is more complex, but it helps with **smooth gradient updates**:

```math
f'(Z) = 0.5 \left(1 + \tanh \left( \sqrt{\frac{2}{\pi}} \left( Z + 0.044715 Z^3 \right) \right) \right) + 0.5 Z \left(1 - \tanh^2 \left( \sqrt{\frac{2}{\pi}} \left( Z + 0.044715 Z^3 \right) \right) \right) \left( \sqrt{\frac{2}{\pi}} \left(1 + 3 \times 0.044715 Z^2 \right) \right)
```

#### **Pros**
✅ Used in **state-of-the-art deep learning models**.  
✅ Helps with training deep architectures.

#### **Cons**
❌ Very computationally expensive.

#### **Best Use Cases**
- If you're working on **transformers** or large **deep networks**.
- If you want to **experiment with cutting-edge techniques**.


#### 🔥 **Summary Table**

| Activation Function | Best For                            | Pros                                                                 | Cons                                                     |
|---------------------|-------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------|
| **ReLU**            | Most deep learning tasks            | Fast, simple, avoids vanishing gradients                             | Can have dead neurons (dying ReLU problem)               |
| **Leaky ReLU**      | Tasks where ReLU fails              | Prevents dying neurons                                               | Slightly more expensive                                  |
| **ELU**             | Faster training, better convergence | No dead neurons, good for deep networks                              | Requires tuning α, slower than ReLU                      |
| **SELU**            | Deep, fully connected networks      | Self-normalizing, reduces need for BatchNorm, good for deep networks | Requires careful weight initialization and Alpha Dropout |
| **Swish**           | Deep networks (Google-developed)    | Can outperform ReLU                                                  | More complex to compute                                  |
| **GELU**            | Transformer models (BERT, GPT)      | Used in state-of-the-art networks                                    | Computationally expensive                                |

---

## **Initialization functions**

### 🚀 **Weight Initialization Techniques in Deep Learning**
Weight initialization is crucial in training deep neural networks because it influences how gradients propagate during backpropagation. Poor initialization can lead to issues like vanishing or exploding gradients.

Below is a **detailed list of weight initialization techniques**, their mathematical background, and the activation functions they work best with.


### 🔹 **1. Zero Initialization (For bias matrix)**
#### **Definition:**
All weights are initialized to **zero**, and biases are typically also initialized to **zero**.

```math
W = 0, \quad b = 0
```

#### **Problems:**
- If all weights are zero, neurons in the same layer will receive the same gradients.
- This symmetry makes neurons learn the same features, making the network ineffective.

#### **Best Used With:** ❌ **Not recommended for deep networks.**
It can be used **only** for bias initialization.


### 🔹 **2. Random Initialization**
#### **Definition:**
Weights are initialized randomly using a uniform or normal distribution:

- **Uniform Distribution**:  
  ```math
  W \sim U(-a, a)
    ```
  
- **Normal Distribution**:  
  ```math
  W \sim \mathcal{N}(0, \sigma^2)
  ```

where $a$ and $\sigma$ are chosen empirically.

#### **Problems:**
- If values are too large → **Exploding gradients**.
- If values are too small → **Vanishing gradients**.

#### **Best Used With:**
✅ Works for shallow networks, but not ideal for deep networks.


### 🔹 **3. Xavier (Glorot) Initialization**
#### **Definition:**
Designed to keep the variance of activations constant across layers. Based on the idea that:

```math
\text{Var}(W \cdot X) = \text{Var}(X)
```

To achieve this, Xavier initialization sets:

- **For uniform distribution:**
  ```math
  W \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}, \frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}\right)
  ```

- **For normal distribution:**
  ```math
  W \sim \mathcal{N}\left(0, \frac{1}{n_{\text{in}} + n_{\text{out}}}\right)
  ```

where:
- $n_{\text{in}}$ = Number of inputs to the neuron
- $n_{\text{out}}$ = Number of outputs from the neuron

#### **Advantages:**
✅ Helps maintain stable variance across layers.  
✅ Prevents vanishing/exploding gradients in deep networks.

#### **Best Used With:**
✅ **Sigmoid, Tanh**  
⏳ **Not ideal for ReLU-based activations** (since ReLU tends to produce asymmetric activations).


### 🔹 **4. He Initialization (Kaiming Initialization)**
#### **Definition:**
Optimized for **ReLU** and its variants. Unlike Xavier, He initialization only considers **input neurons** because ReLU deactivates half of the neurons.

- **For uniform distribution:**
  ```math
  W \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{\text{in}}}}, \frac{\sqrt{6}}{\sqrt{n_{\text{in}}}}\right)
  ```

- **For normal distribution:**
  ```math
  W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
  ```

#### **Advantages:**
✅ Keeps activations from shrinking or exploding.  
✅ Works well with ReLU-based activations.

#### **Best Used With:**
✅ **ReLU, Leaky ReLU, ELU**  
❌ **Not ideal for Sigmoid/Tanh** (because it leads to large activation outputs and saturation).


### 🔹 **5. LeCun Initialization**
#### **Definition:**
Specialized for **Sigmoid and Tanh**, where weights are initialized to:

- **For uniform distribution:**
  ```math
  W \sim U\left(-\frac{\sqrt{3}}{\sqrt{n_{\text{in}}}}, \frac{\sqrt{3}}{\sqrt{n_{\text{in}}}}\right)
  ```

- **For normal distribution:**
  ```math
  W \sim \mathcal{N}\left(0, \frac{1}{n_{\text{in}}}\right)
  ```

#### **Advantages:**
✅ Works well for networks using **Tanh and Sigmoid**.  
✅ Helps prevent saturation.

#### **Best Used With:**
✅ **Tanh, Sigmoid**  
⏳ **Not ideal for ReLU** (He initialization is better).


### 🔹 **6. SELU (Scaled Exponential Linear Unit) Initialization**
#### **Definition:**
Designed for **Self-Normalizing Networks** (SNNs). SELU has a unique property:  
✅ It **automatically normalizes activations** across layers.

Weights follow:

```math
W \sim \mathcal{N} \left(0, \frac{1}{n_{\text{in}}} \right)
```

and biases should be **zero**.

#### **Advantages:**
✅ Allows deep networks to **self-normalize**.  
✅ Eliminates need for Batch Normalization.

#### **Best Used With:**
✅ **SELU Activation**  
❌ **Not recommended for ReLU, Sigmoid, or Tanh**.


### 🔥 **Summary Table**
| **Initialization**  | **Best For**                 | **Formula**                                                             | **Best Activation Functions**             |
|---------------------|------------------------------|-------------------------------------------------------------------------|-------------------------------------------|
| **Zero Init**       | Bias Matrix                  | $\( W = 0 \)$                                                           | ✅ Bias Matrix                             |
| **Random Init**     | Small networks               | $\( W \sim U(-a, a) \) or \( W \sim \mathcal{N}(0, \sigma^2) \)$        | ✅ Any (but not optimal for deep networks) |
| **Xavier (Glorot)** | Avoiding vanishing gradients | $\( W \sim \mathcal{N}(0, \frac{1}{n_{\text{in}} + n_{\text{out}}}) \)$ | ✅ Sigmoid, Tanh                           |
| **He Init**         | ReLU-based activations       | $\( W \sim \mathcal{N}(0, \frac{2}{n_{\text{in}}}) \)$                  | ✅ ReLU, Leaky ReLU, ELU, Swish, GELU      |
| **LeCun Init**      | Self-normalizing nets        | $\( W \sim \mathcal{N}(0, \frac{1}{n_{\text{in}}}) \)$                  | ✅ Sigmoid, Tanh                           |
| **SELU Init**       | Self-Normalizing Nets        | $\( W \sim \mathcal{N}(0, \frac{1}{n_{\text{in}}}) \)$                  | ✅ SELU                                    |

### **Best Initialization for Each Activation**
| Activation     | Best Initialization                                                      |
|----------------|--------------------------------------------------------------------------|
| **Softmax**    | Xavier Glorot Normal, Xavier Glorot Uniform                              |
| **Sigmoid**    | Xavier Glorot Normal, Xavier Glorot Uniform                              |
| **Tanh**       | Xavier Glorot Normal, Xavier Glorot Uniform, Lecun Normal, Lecun Uniform |
| **ReLU**       | He Normal, He Uniform                                                    |
| **Leaky ReLU** | He Normal, He Uniform                                                    |
| **ELU**        | He Normal, He Uniform                                                    |
| **SELU**       | Lecun Normal, Lecun Uniform, SELU Initialization                         |
| **Swish**      | He Normal                                                                |
| **GELU**       | He Normal                                                                |

---

## **Optimization Functions**

The project supports multiple **optimization algorithms**, each designed to adjust neural network weights effectively during training. Below is an overview of the **implemented optimization functions**, their properties, and when to use them.


### **1. Gradient Descent (GD)**
#### **Definition**
Gradient Descent is the most fundamental optimization method. It updates weights by moving in the **opposite direction of the gradient** of the loss function.

**Update Rule:**  
```math
W = W - \alpha \cdot \nabla L(W)
```
where:
- $W$ are the model parameters,
- $\alpha$ is the learning rate,
- $\nabla L(W)$ is the gradient of the loss function with respect to \( W \).

**Key Features:**  
✅ Simple and easy to implement.  
✅ Works well for convex optimization problems.  
❌ Can be **slow** for large datasets.  
❌ Requires **manual tuning** of the learning rate.

📌 **Best Used When:**
- The dataset is small.
- The loss function is convex.
- You don’t need adaptive learning rates.


### **2. AdaGrad (Adaptive Gradient Algorithm)**
#### **Definition**
AdaGrad adapts the learning rate for each parameter based on past gradients. It assigns **larger updates to infrequent parameters** and **smaller updates to frequent ones**.

**Update Rule:**  
```math
G_t = G_{t-1} + (\nabla L(W))^2
```
```math
W = W - \frac{\alpha}{\sqrt{G_t} + \epsilon} \cdot \nabla L(W)
```

where $G_t$ is the accumulated sum of past squared gradients.

**Key Features:**  
✅ Good for **sparse data** and **natural language processing (NLP)** tasks.  
✅ No need to manually adjust the learning rate.  
❌ Learning rate **decreases too much over time**, leading to early convergence.

📌 **Best Used When:**
- Working with **sparse data** (e.g., NLP, recommendation systems).
- Handling **features with different scales**.


### **3. Momentum Optimization**
#### **Definition**
Momentum optimization **accelerates gradient descent** by maintaining a moving average of past gradients. Instead of just considering the current gradient, it accumulates past gradients to create a **velocity term**.

**Update Rule:**  
```math
V_t = \beta V_{t-1} + (1 - \beta) \nabla L(W)
```
```math
W = W - \alpha \cdot V_t
```

where:
- $\beta$ (default: **0.9**) controls how much past gradients influence the update.
- $V_t$ is the velocity term.

**Key Features:**  
✅ **Faster convergence** than standard Gradient Descent.  
✅ Reduces oscillations in deep networks.  
❌ Requires **tuning the momentum factor** \( \beta \).

📌 **Best Used When:**
- Training **deep neural networks**.
- Avoiding oscillations in **high-dimensional spaces**.


### **4. RMSprop (Root Mean Square Propagation)**
#### **Definition**
RMSprop improves AdaGrad by **controlling the learning rate decay**. It maintains an **exponentially weighted moving average** of past squared gradients, preventing the learning rate from dropping too quickly.

**Update Rule:**  
```math
V_t = \beta V_{t-1} + (1 - \beta) (\nabla L(W))^2
```
```math
W = W - \frac{\alpha}{\sqrt{V_t} + \epsilon} \cdot \nabla L(W)
```

where:
- $V_t$ is the moving average of squared gradients.
- $\beta$ (default: **0.9**) controls decay speed.

**Key Features:**  
✅ Works well for **non-stationary objectives** (e.g., RL, dynamic datasets).  
✅ Avoids **AdaGrad’s rapid learning rate decay**.  
❌ Requires **tuning \( \beta \)** for stability.

📌 **Best Used When:**
- Working with **online learning** and **reinforcement learning**.
- Handling **non-stationary data** (e.g., time series).


### **5. Adam (Adaptive Moment Estimation)**
#### **Definition**
Adam combines **Momentum** and **RMSprop** to get the **benefits of both**. It maintains:
1. An exponentially weighted moving average of past gradients (**Momentum**).
2. An exponentially weighted moving average of squared gradients (**RMSprop**).

**Update Rule:**  
```math
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L(W)
```
```math
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L(W))^2
```
```math
W = W - \frac{\alpha}{\sqrt{v_t} + \epsilon} \cdot m_t
```

where:
- $\beta_1$ (default: **0.9**) controls momentum.
- $\beta_2$ (default: **0.999**) controls adaptive learning rate.

**Key Features:**  
✅ Works well **out of the box** with minimal tuning.  
✅ Adaptively adjusts learning rates.  
❌ Can lead to **poor generalization** (overfitting).

📌 **Best Used When:**
- Training **deep neural networks**.
- Working with **large datasets**.
- Handling **complex optimization landscapes**.


### **6. AdamW (Adam with Weight Decay)**
#### **Definition**
AdamW improves **Adam** by **decoupling weight decay from the learning rate**. Instead of applying **L2 regularization** via the learning rate, it applies explicit **weight decay**. Therefore, L2 regularization should be disabled when using AdamW.

**Update Rule:**  
```math
W = W - \alpha \cdot \frac{m_t}{\sqrt{v_t} + \epsilon} - \alpha \lambda W
```

where:
- $\lambda$ is the **weight decay** factor (default: **0.01**).
- Other terms are the same as Adam.

**Key Features:**  
✅ **Better generalization** than Adam.  
✅ Effectively **controls overfitting**.  
❌ Still requires **careful tuning** of \( \lambda \) and learning rate.

📌 **Best Used When:**
- Training **large models** (e.g., transformers, deep CNNs).
- Preventing **overfitting** in Adam-based optimizers.


## **Comparison Table**
| Optimizer              | Learning Rate Adaptation | Momentum | Handles Sparse Data | Best For                           |
|------------------------|--------------------------|----------|---------------------|------------------------------------|
| **Gradient Descent**   | ❌ No                     | ❌ No     | ❌ No                | Small datasets                     |
| **AdaGrad**            | ✅ Yes                    | ❌ No     | ✅ Yes               | NLP, sparse data                   |
| **Momentum**           | ❌ No                     | ✅ Yes    | ❌ No                | Deep networks                      |
| **RMSprop**            | ✅ Yes                    | ✅ Yes    | ❌ No                | Non-stationary data                |
| **Adam**               | ✅ Yes                    | ✅ Yes    | ✅ Yes               | General-purpose optimization       |
| **AdamW**              | ✅ Yes                    | ✅ Yes    | ✅ Yes               | Large models, avoiding overfitting |

---

## **Learning Rate Schedules**

Learning rate schedules dynamically adjust the **learning rate** during training to improve convergence and performance. This section explains the **implemented learning rate schedules**, how they work, and when to use them.


### **1. Step Decay**
#### **Definition**
Step decay **reduces the learning rate by a fixed factor** after a certain number of epochs. This allows the model to learn quickly in the early stages and fine-tune in later stages.

**Update Rule:**  
```math
\alpha_t = \alpha_0 \times (\text{drop factor})^{\left\lfloor \frac{\text{epoch}}{\text{epochs drop}} \right\rfloor}
```
where:
- $\alpha_0$ is the initial learning rate.
- **drop factor** (default: **0.1**) is the rate at which learning decreases.
- **epochs drop** (default: **50**) is how often the learning rate decreases.

📌 **Example Behavior:** If **drop factor = 0.1** and **epochs drop = 50**, the learning rate drops **10x every 50 epochs**.

**Key Features:**  
✅ Keeps learning fast at the beginning.  
✅ Helps fine-tune in later training stages.  
❌ **Abrupt drops** may destabilize training.

📌 **Best Used When:**
- The dataset is **well-structured**.
- You want a simple **scheduled learning rate reduction**.


### **2. Exponential Decay**
#### **Definition**
Exponential decay **reduces the learning rate continuously** by an exponential factor, rather than in discrete steps.

**Update Rule:**  
```math
\alpha_t = \alpha_0 \times e^{-\lambda \cdot t}
```
where:
- $\alpha_0$ is the initial learning rate.
- $\lambda$ (default: **0.01**) is the **decay rate**.
- $t$ is the current epoch.

📌 **Example Behavior:** If **decay rate = 0.01**, the learning rate **gradually decreases over time**.

**Key Features:**  
✅ Smoother than step decay.  
✅ Helps stabilize training in later epochs.  
❌ May **shrink the learning rate too quickly** if decay is too high.

📌 **Best Used When:**
- Training deep networks.
- You want **gradual decay** instead of abrupt drops.


### **3. Time-Based Decay**
#### **Definition**
Time-based decay **reduces the learning rate over time** using a **divisor**.

**Update Rule:**  
```math
\alpha_t = \frac{\alpha_0}{1 + \lambda \cdot t}
```
where:
- $\alpha_0$ is the initial learning rate.
- $\lambda$ (default: **0.01**) is the **decay rate**.
- $t$ is the epoch number.

📌 **Example Behavior:** If **decay rate = 0.01**, the learning rate decreases **linearly** over time.

**Key Features:**  
✅ Smooth and simple decay.  
✅ Reduces learning rate **without abrupt changes**.  
❌ May decay too fast, **causing underfitting**.

📌 **Best Used When:**
- You have **many epochs** (e.g., **>1000**).
- You want a **controlled, predictable decay**.


### **4. Cosine Annealing**
#### **Definition**
Cosine annealing **reduces the learning rate using a cosine function**. It starts high, decreases smoothly, and **can restart** at higher values.

**Update Rule:**  
```math
\alpha_t = \frac{\alpha_0}{2} \times \left(1 + \cos\left(\frac{\pi t}{T}\right)\right)
```
where:
- $\alpha_0$ is the initial learning rate.
- $T$ is the total number of epochs.
- $t$ is the current epoch.

📌 **Example Behavior:** The learning rate **oscillates**, starting high, decreasing to near zero, and possibly **restarting**.

**Key Features:**  
✅ Prevents getting stuck in local minima.  
✅ Works well with **adaptive learning rates**.  
❌ May be **complex to tune**.

📌 **Best Used When:**
- Training **transformers, CNNs, or deep networks**.
- You want to **avoid local minima**.


### **Comparison Table**
| Schedule Type         | Formula                                                                                                              | Behavior          | Best For                          |
|-----------------------|----------------------------------------------------------------------------------------------------------------------|-------------------|-----------------------------------|
| **Step Decay**        | $\alpha_t = \alpha_0 \times \text{drop factor}^{\left\lfloor \frac{\text{epoch}}{\text{epochs drop}} \right\rfloor}$ | Sudden drops      | Stable datasets, structured decay |
| **Exponential Decay** | $\alpha_t = \alpha_0 \times e^{-\lambda \cdot t}$                                                                    | Smooth decrease   | Deep networks, gradual decay      |
| **Time-Based Decay**  | $\alpha_t = \frac{\alpha_0}{1 + \lambda \cdot t}$                                                                    | Linear decrease   | Many epochs, controlled decay     |
| **Cosine Annealing**  | $\alpha_t = \frac{\alpha_0}{2} \times (1 + \cos(\frac{\pi t}{T}))$                                                   | Oscillating       | Avoiding local minima             |


### **Choosing the Right Learning Rate Schedule**

- If training is **too slow**, use **Step Decay**.
- If learning **decreases too fast**, use **Exponential Decay**.
- If you want a **smooth decrease**, use **Time-Based Decay**.
- If you **suspect local minima**, try **Cosine Annealing**.

---

## **Evaluation Metrics**

Evaluation metrics are essential for assessing the **performance of a machine learning model**. This section describes the **implemented metrics**, their mathematical formulas, and when to use them.


### **1. Accuracy**
#### **Definition**
Accuracy is the **ratio of correctly predicted samples** to the total number of samples.

**Formula:**  
```math
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Samples}}
```

**Key Features:**  
✅ Simple and easy to interpret.  
✅ Useful when **class distribution is balanced**.  
❌ **Misleading for imbalanced datasets** (e.g., if 95% of samples belong to one class, a model that always predicts the majority class will have 95% accuracy but is useless).

#### **Interpretation**
- **High Accuracy (close to 1.0 or 100%)** → Model is correctly classifying most samples.
- **Low Accuracy (close to 0.5 or lower)** → Model is not performing better than random guessing.
- **Danger**: Accuracy can be **misleading on imbalanced datasets**.  

📌 **Best Used When:**
- **Classes are balanced** (roughly equal number of positive and negative samples).


### **2. Precision**
#### **Definition**
Precision measures the **proportion of correctly identified positive samples** out of all predicted positives.

**Formula:**  
```math
\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
```

where:
- **True Positives (TP):** Correctly predicted positive samples.
- **False Positives (FP):** Incorrectly predicted positive samples.

**Key Features:**  
✅ Important when **False Positives are costly** (e.g., spam detection, medical tests).  
✅ Helps when you **want to avoid misclassifying negative cases as positive**.  
❌ Does not account for **False Negatives (FN)**.

#### **Interpretation**
- **High Precision (close to 1.0)** → When the model predicts **positive**, it is **usually correct**.
- **Low Precision (close to 0.0)** → Many **false positives** (incorrect positive predictions).
- **Tradeoff**: Increasing Precision often **decreases Recall**.

📌 **Best Used When:**
- **False positives are a bigger concern** (e.g., fraud detection, email spam classification).


### **3. Recall (Sensitivity, True Positive Rate)**
#### **Definition**
Recall measures how many **actual positive samples** were correctly classified.

**Formula:**  
```math
\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
```

where:
- **False Negatives (FN):** Cases that were actually positive but incorrectly classified as negative.

**Key Features:**  
✅ Important when **False Negatives are costly** (e.g., cancer detection, safety-critical applications).  
✅ Helps when you **want to minimize missing positive cases**.  
❌ May increase **False Positives**, reducing Precision.

#### **Interpretation**
- **High Recall (close to 1.0)** → The model captures **most positive cases** but might include **some false positives**.
- **Low Recall (close to 0.0)** → Many **true positives are missed**, meaning **False Negatives are high**.
- **Tradeoff**: Increasing Recall often **decreases Precision**.

📌 **Best Used When:**
- **False negatives are a bigger concern** (e.g., medical diagnoses, fraud detection).


### **4. F1 Score (Harmonic Mean of Precision & Recall)**
#### **Definition**
F1 Score is the **harmonic mean of Precision and Recall**, balancing both metrics.

**Formula:**  
```math
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
```

**Key Features:**  
✅ Useful for **imbalanced datasets**.  
✅ Ensures a balance between **Precision and Recall**.  
❌ Less interpretable than Accuracy.

#### **Interpretation**
- **High F1 Score (close to 1.0)** → Model maintains a good **balance** between Precision and Recall.
- **Low F1 Score (close to 0.0)** → Model struggles with both False Positives and False Negatives.
- **Best Metric for** **imbalanced datasets**.

📌 **Best Used When:**
- **Class distribution is imbalanced** (e.g., rare event classification).


### **5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**
#### **Definition**
ROC-AUC measures how well the model separates **positive and negative classes**.

- **ROC Curve:** Plots **True Positive Rate (Recall)** vs. **False Positive Rate (FPR)**.
- **AUC (Area Under Curve):** The area beneath the ROC curve, ranging from **0 to 1**.
  - 1.0 = Perfect classifier
  - 0.5 = Random guessing

**Key Features:**  
✅ Useful for **binary classification tasks**.  
✅ Works well with **imbalanced datasets**.  
❌ Can be misleading when the **cost of False Positives and False Negatives differs**.

📌 **Best Used When:**
- Evaluating **binary classifiers with probabilistic outputs**.


### **6. PR-AUC (Precision-Recall Area Under Curve)**
#### **Definition**
PR-AUC measures the **area under the Precision-Recall curve**, focusing on the model’s performance for **positive class predictions**.

- **Precision-Recall Curve:** Plots **Precision** vs. **Recall**.
- **AUC (Area Under Curve):** Measures the model’s ability to maintain **high precision at various recall levels**.
  - 1.0 = Model balances Precision & Recall well
  - 0.5 = Model is near random guessing

**Key Features:**  
✅ **Better than ROC-AUC for imbalanced datasets**.  
✅ Helps in **cases where Precision and Recall matter more than overall accuracy**.  
❌ Less useful for **balanced datasets**.

📌 **Best Used When:**
- **Class distribution is highly skewed** (e.g., fraud detection, rare diseases).


## **Comparison Table**
| Metric          | Formula                                                                                   | Best For                   | Weakness                            |
|-----------------|-------------------------------------------------------------------------------------------|----------------------------|-------------------------------------|
| **Accuracy**    | $\frac{TP + TN}{TP + TN + FP + FN}$                                                       | Balanced datasets          | Fails on imbalanced data            |
| **Precision**   | $\frac{TP}{TP + FP}$                                                                      | Avoiding False Positives   | Ignores False Negatives             |
| **Recall**      | $\frac{TP}{TP + FN}$                                                                      | Avoiding False Negatives   | Increases False Positives           |
| **F1 Score**    | $2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | Imbalanced data            | Less intuitive                      |
| **ROC-AUC**     | Area under ROC curve                                                                      | Binary classification      | Fails if class imbalance is extreme |
| **PR-AUC**      | Area under PR curve                                                                       | Imbalanced datasets        | Less useful for balanced data       |


## **Choosing the Right Metric**

| Scenario                                                | Recommended Metric |
|---------------------------------------------------------|--------------------|
| **Balanced dataset**                                    | Accuracy           |
| **False Positives are costly** (e.g., spam detection)   | Precision          |
| **False Negatives are costly** (e.g., cancer detection) | Recall             |
| **Overall performance on imbalanced data**              | F1 Score           |
| **Binary classification with probability outputs**      | ROC-AUC            |
| **Highly imbalanced datasets**                          | PR-AUC             |

---

## **Regularization Techniques**

Regularization is a set of techniques used to **prevent overfitting** in machine learning models. This section explains the **implemented regularization methods**, their effects, and when to use them.


### **1. L1 and L2 Regularization**

L1 and L2 regularization are techniques used in deep neural networks (DNNs) to prevent overfitting by adding a penalty term to the loss function.

#### **L1 Regularization (Lasso)**
- L1 regularization adds the **absolute values** of the weights to the loss function.
- The modified loss function for L1 regularization:
  ```math
  \mathcal{L}_{\text{L1}} = \mathcal{L}_{\text{original}} + \lambda \sum |w_i|
  ```
  where:
  - $\mathcal{L}_{\text{original}}$ is the original loss,
  - $w_i$ are the model weights,
  - $\lambda$ is a regularization parameter that controls the penalty's strength.

#### **Effects of L1 Regularization**
- Encourages **sparsity**, meaning it forces some weights to become exactly **zero**, effectively performing feature selection.
- Useful when dealing with high-dimensional data with many irrelevant features.


#### **L2 Regularization (Ridge)**
- L2 regularization adds the **squared values** of the weights to the loss function.
- The modified loss function for L2 regularization:
  ```math
  \mathcal{L}_{\text{L2}} = \mathcal{L}_{\text{original}} + \lambda \sum w_i^2
  ```

#### **Effects of L2 Regularization**
- Encourages smaller but nonzero weights, preventing excessive weight values (shrinking them towards zero but not exactly zero).
- Helps distribute the importance across all features rather than eliminating some entirely.
- Improves generalization by reducing variance.


#### **Comparison of L1 vs. L2 Regularization**
| Feature                  | L1 Regularization (Lasso)                            | L2 Regularization (Ridge)                       |
|--------------------------|------------------------------------------------------|-------------------------------------------------|
| Effect on weights        | Promotes sparsity (some weights become exactly zero) | Shrinks weights smoothly but keeps them nonzero |
| Use case                 | Feature selection, high-dimensional data             | General regularization, avoids large weights    |
| Computational efficiency | Less computationally expensive                       | More computationally expensive                  |
| Typical value range      | Smaller than L2                                      | 0.001 -> 0.00001                                |


#### **L1 + L2: Elastic Net**
- A combination of both L1 and L2 is called **Elastic Net Regularization**:
  ```math
  \mathcal{L}_{\text{ElasticNet}} = \mathcal{L}_{\text{original}} + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2
  ```
- Helps balance sparsity and weight shrinkage for better performance.


### **2. Dropout**

Dropout is a regularization technique used in deep neural networks to prevent overfitting by randomly "dropping out" (i.e., setting to zero) a fraction of neurons during training. This forces the network to learn more robust and generalizable features.
Excessive dropout may lead to **underfitting**.

#### **How It Works**
- During **each training iteration**, a subset of neurons is randomly deactivated (along with their connections).
- This prevents the network from relying too heavily on any single neuron, encouraging a **more distributed representation of features**.
- At **inference time**, no neurons are dropped, but their outputs are scaled down by the dropout rate to compensate for the training phase. If activations are not scaled, the network will receive a higher signal strength than during training, which can lead to incorrect predictions.

Example: dropout rate of 0.2 means 20% of neurons are dropped.


#### **Why Dropout Works**
- **Prevents co-adaptation:** Forces neurons to work independently instead of relying on specific neurons.
- **Reduces overfitting:** Since different subsets of neurons are used in each training step, the network generalizes better.
- **Acts as an ensemble method:** Each forward pass during training can be seen as training a slightly different subnetwork. The final model is an average of these subnetworks, improving robustness.


#### **Choosing a Dropout Rate**
- **Shallow networks** (1-2 layers): **0.1 - 0.3**
- **Deep networks** (many layers): **0.3 - 0.5**
- **Convolutional networks (CNNs)**: **0.1 - 0.3** (dropout is usually applied after fully connected layers)
- **Recurrent networks (RNNs, LSTMs)**: **0.1 - 0.3** (applied to input/output connections)


### **3. Early Stopping**

#### **What is Early Stopping?**
Early stopping is a **regularization technique** that helps prevent overfitting by **stopping training when the validation performance starts to degrade**. Instead of training a deep neural network for a fixed number of epochs, early stopping monitors the model's performance on a validation set and stops training when performance stops improving.


#### **How Early Stopping Works**
1. **Split the dataset:** The dataset is divided into **training** and **validation** sets.
2. **Monitor validation loss:** During training, the model's performance is evaluated on the validation set at the end of each epoch.
3. **Detect overfitting:** If the validation loss starts increasing (while training loss keeps decreasing), the model is **overfitting**.
4. **Stop training:** The training stops when the validation loss does not improve for a certain number of epochs (patience parameter).
5. **Use the best model:** The model is rolled back to the point where it had the best validation performance.


#### **Why Use Early Stopping?**
- **Prevents overfitting:** Stops training before the model memorizes noise in the training data.
- **Saves time & resources:** Avoids unnecessary computations by halting training early.
- **Dynamic regularization:** Unlike L1/L2 or dropout, early stopping dynamically adjusts training duration.


#### **Choosing the Right `patience` Value**
- **Too small (`patience = 1-2`)** → Might stop too early before the model has learned enough.
- **Too large (`patience > 10`)** → Might not prevent overfitting effectively.
- **Typical values:** `patience = 5-10`